{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "436319bd",
      "metadata": {
        "id": "436319bd"
      },
      "source": [
        "# Week 3 seminar: Graph Neural Networks\n",
        "\n",
        "Graph Neural Networks (GNNs) are currently the most popular approach to machine learning on graphs. Many GNN architectures can be unified by the Message-Passing Neural Networks (MPNNs) framework. Below we will describe (a variant of) this framework and implement and train several examples of MPNNs.\n",
        "\n",
        "First, let's introduce the notation we will be using in this notebook. Let $G = (V, E)$ be a simple undirected graph without self-loops with node set $V$ and edge set $E$, $|V| = n$, $|E| = m$. Sometimes it will also be handy to use the set $E_{dir}$ of directed edges, $|E_{dir}| = 2m$. Let $N(v)$ be the set of one-hop neighbors of node $v$, and $deg(v)$ be the degree of node $v$, $deg(v) = |N(v)|$. Let $A$ be the adjacency matrix of graph $G$ and $D$ be the diagonal degree matrix of graph $G$, i.e., $D = diag \\Big( deg(v_1), \\; deg(v_2), \\; ..., \\; deg(v_n) \\Big)$.\n",
        "\n",
        "In each layer $l$ an MPNN creates a representation $x_i^l$ of each node $v_i$ from it's previous-layer representation $x_i^{l-1}$ and previous-layer representations of its neighbors. The formula for this transformation at layer $l+1$ is:\n",
        "\n",
        "$$ x_i^{l+1} = \\mathrm{Update} \\Bigg( x_i^l, \\; \\mathrm{Aggregate} \\Big( \\Big\\{ (x_i^l, \\; x_j^l): \\; v_j \\in N(v_i) \\Big\\} \\Big) \\Bigg) $$\n",
        "\n",
        "Here, $\\mathrm{Aggregate}$ is a function that aggregates information from the set of neighbors (since it operates on a set, it should be invariant to the order of neighbors) and $\\mathrm{Update}$ is a function that combines the node's previous-layer representation with the aggregated information from its neighbors. For example, $\\mathrm{Aggregate}$ can be the elementwise mean operation over the set of neighbors and $\\mathrm{Update}$ can be an MLP that takes two concatenated vectors as input:\n",
        "\n",
        "$$ x_i^{l+1} = \\mathrm{MLP} \\Bigg( \\bigg[ x_i^l \\; \\mathbin\\Vert \\; \\mathrm{mean} \\Big( \\Big\\{ x_j^l: \\; v_j \\in N(v_i) \\Big\\} \\Big) \\bigg] \\Bigg) $$\n",
        "\n",
        "(this is actually the first GNN that we will implement in this seminar).\n",
        "\n",
        "The $\\mathrm{Aggregate}$ operation is often called message passing, neighborhood aggregation, or graph convolution. Sometimes this operation is split into $\\mathrm{Message}$ and $\\mathrm{Reduce}$ functions.\n",
        "\n",
        "Note that variations of the above MPNN formula are possible. For example, edge representations can be added, but we won't do it in this seminar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "b5f6034e",
      "metadata": {
        "id": "b5f6034e"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "di = torch.zeros(graph.shape[0])\n",
        "di"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq6vPqbymh4N",
        "outputId": "f5b7ff32-9784-432c-a823-d1dc82fa941a"
      },
      "id": "Lq6vPqbymh4N",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph.cpu()[range(10)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_huevO4VoQFm",
        "outputId": "53d2e56e-f2ec-4770-abfc-1983f5df4cb4"
      },
      "id": "_huevO4VoQFm",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Could not run 'aten::index.Tensor' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index.Tensor' is only available for these backends: [CPU, CUDA, HIP, MPS, IPU, XPU, HPU, VE, MTIA, PrivateUse1, PrivateUse2, PrivateUse3, Meta, FPGA, ORT, Vulkan, Metal, QuantizedCPU, QuantizedCUDA, QuantizedHIP, QuantizedMPS, QuantizedIPU, QuantizedXPU, QuantizedHPU, QuantizedVE, QuantizedMTIA, QuantizedPrivateUse1, QuantizedPrivateUse2, QuantizedPrivateUse3, QuantizedMeta, CustomRNGKeyId, MkldnnCPU, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nUndefined: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31357 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44411 [kernel]\nHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nVE: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMTIA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMeta: registered at /dev/null:241 [kernel]\nFPGA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nORT: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nVulkan: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMetal: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedVE: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedMTIA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedMeta: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nCustomRNGKeyId: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:324 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16002 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1242 [kernel]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-344c3e7dbbac>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::index.Tensor' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index.Tensor' is only available for these backends: [CPU, CUDA, HIP, MPS, IPU, XPU, HPU, VE, MTIA, PrivateUse1, PrivateUse2, PrivateUse3, Meta, FPGA, ORT, Vulkan, Metal, QuantizedCPU, QuantizedCUDA, QuantizedHIP, QuantizedMPS, QuantizedIPU, QuantizedXPU, QuantizedHPU, QuantizedVE, QuantizedMTIA, QuantizedPrivateUse1, QuantizedPrivateUse2, QuantizedPrivateUse3, QuantizedMeta, CustomRNGKeyId, MkldnnCPU, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nUndefined: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31357 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44411 [kernel]\nHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nVE: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMTIA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMeta: registered at /dev/null:241 [kernel]\nFPGA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nORT: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nVulkan: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMetal: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedVE: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedMTIA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedMeta: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nCustomRNGKeyId: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:324 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16002 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1242 [kernel]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(graph.shape[0]):\n",
        "    di[i] = graph[i][i]"
      ],
      "metadata": {
        "id": "ZX5CodYGn-s1"
      },
      "id": "ZX5CodYGn-s1",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "44bc10fc",
      "metadata": {
        "id": "44bc10fc"
      },
      "outputs": [],
      "source": [
        "device = 'cuda:0'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09cc361f",
      "metadata": {
        "id": "09cc361f"
      },
      "source": [
        "Now, let's get us a graph. PyTorch Geometric library provides a lot of popular graph datasets. We will use the Amazon-Computers dataset. It is a co-purchasing network where nodes represent products, edges indicate that two products are frequently bought together, node features are bag-of-words-encoded product reviews, and node labels are product categories. Note that this graph has multiple connected components and some isolated nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e73406c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e73406c6",
        "outputId": "93cd2a78-346a-48b6-a521-ba4600118364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m0.9/1.1 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.4.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ab958983",
      "metadata": {
        "id": "ab958983"
      },
      "outputs": [],
      "source": [
        "from torch_geometric import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bfd0e436",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfd0e436",
        "outputId": "9bdcc616-354b-4a21-aa09-946a0a7ab7c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/shchur/gnn-benchmark/raw/master/data/npz/amazon_electronics_computers.npz\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 13752\n",
            "Number of edges: 245861\n",
            "Average node degree: 35.76\n",
            "Number of classes: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "data = datasets.Amazon(name='computers', root='data')[0]\n",
        "features = data.x\n",
        "labels = data.y\n",
        "edges = data.edge_index.T\n",
        "\n",
        "# The graph is undirected, but is stored as a directed one (like all graphs in PyTorch Geometric),\n",
        "# so each edge appears twice.\n",
        "print(f'Number of nodes: {len(labels)}')\n",
        "print(f'Number of edges: {len(edges) // 2}')\n",
        "print(f'Average node degree: {len(edges) / len(labels):.2f}')\n",
        "print(f'Number of classes: {len(labels.unique())}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c05695fb",
      "metadata": {
        "id": "c05695fb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d40cf2fc",
      "metadata": {
        "id": "d40cf2fc"
      },
      "outputs": [],
      "source": [
        "full_idx = np.arange(len(labels))\n",
        "train_idx, val_and_test_idx = train_test_split(full_idx, test_size=0.5, random_state=0,\n",
        "                                               stratify=labels)\n",
        "\n",
        "val_idx, test_idx = train_test_split(val_and_test_idx, test_size=0.5, random_state=0,\n",
        "                                     stratify=labels[val_and_test_idx])\n",
        "\n",
        "train_idx = torch.from_numpy(train_idx)\n",
        "val_idx = torch.from_numpy(val_idx)\n",
        "test_idx = torch.from_numpy(test_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44d27aa1",
      "metadata": {
        "id": "44d27aa1"
      },
      "source": [
        "Let's prepare a training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "f2f2bb9b",
      "metadata": {
        "id": "f2f2bb9b"
      },
      "outputs": [],
      "source": [
        "def train_step(model, loss_fn, optimizer, scaler, amp, graph, features, labels, train_idx):\n",
        "    model.train()\n",
        "\n",
        "    with autocast(enabled=amp):\n",
        "        logits = model(graph=graph, x=features).squeeze(1)\n",
        "        loss = loss_fn(input=logits[train_idx], target=labels[train_idx])\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, metric, amp, graph, features, labels, train_idx, test_idx, val_idx):\n",
        "    model.eval()\n",
        "\n",
        "    with autocast(enabled=amp):\n",
        "        logits = model(graph=graph, x=features)\n",
        "\n",
        "    if metric == 'ROC AUC':\n",
        "        labels = labels.cpu().numpy()\n",
        "        logits = logits.cpu().numpy()\n",
        "        train_idx = train_idx.cpu().numpy()\n",
        "        val_idx = val_idx.cpu().numpy()\n",
        "        test_idx = test_idx.cpu().numpy()\n",
        "\n",
        "        train_metric = roc_auc_score(y_true=labels[train_idx], y_score=logits[train_idx]).item()\n",
        "        val_metric = roc_auc_score(y_true=labels[val_idx], y_score=logits[val_idx]).item()\n",
        "        test_metric = roc_auc_score(y_true=labels[test_idx], y_score=logits[test_idx]).item()\n",
        "\n",
        "    elif metric == 'accuracy':\n",
        "        preds = logits.argmax(axis=1)\n",
        "\n",
        "        train_metric = (preds[train_idx] == labels[train_idx]).float().mean().item()\n",
        "        val_metric = (preds[val_idx] == labels[val_idx]).float().mean().item()\n",
        "        test_metric = (preds[test_idx] == labels[test_idx]).float().mean().item()\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f'Unknown metric: {metric}.')\n",
        "\n",
        "    metrics = {\n",
        "        f'train {metric}': train_metric,\n",
        "        f'val {metric}': val_metric,\n",
        "        f'test {metric}': test_metric\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def run_experiment(graph, features, labels, train_idx, val_idx, test_idx, graph_conv_module, num_layers=2,\n",
        "                   hidden_dim=256, num_heads=4, dropout=0.2, lr=3e-5, num_steps=500, device='cuda:0', amp=False):\n",
        "    num_classes = len(labels.unique())\n",
        "    loss_fn = F.binary_cross_entropy_with_logits if num_classes == 2 else F.cross_entropy\n",
        "    metric = 'ROC AUC' if num_classes == 2 else 'accuracy'\n",
        "    if num_classes == 2:\n",
        "        labels = labels.float()\n",
        "\n",
        "    model = Model(graph_conv_module=graph_conv_module,\n",
        "                  num_layers=num_layers,\n",
        "                  input_dim=features.shape[1],\n",
        "                  hidden_dim=hidden_dim,\n",
        "                  output_dim=1 if num_classes == 2 else num_classes,\n",
        "                  num_heads=num_heads,\n",
        "                  dropout=dropout)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scaler = GradScaler(enabled=amp)\n",
        "\n",
        "    graph = graph.to(device)\n",
        "    features = features.to(device)\n",
        "    labels = labels.to(device)\n",
        "    train_idx = train_idx.to(device)\n",
        "    val_idx = val_idx.to(device)\n",
        "    test_idx = test_idx.to(device)\n",
        "\n",
        "    best_val_metric = 0\n",
        "    corresponding_test_metric = 0\n",
        "    best_step = None\n",
        "    with tqdm(total=num_steps) as progress_bar:\n",
        "        for step in range(1, num_steps + 1):\n",
        "            train_step(model=model, loss_fn=loss_fn, optimizer=optimizer, scaler=scaler, amp=amp, graph=graph,\n",
        "                       features=features, labels=labels, train_idx=train_idx)\n",
        "            metrics = evaluate(model=model, metric=metric, amp=amp, graph=graph, features=features, labels=labels,\n",
        "                               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx)\n",
        "\n",
        "            progress_bar.update()\n",
        "            progress_bar.set_postfix({metric: f'{value:.2f}' for metric, value in metrics.items()})\n",
        "\n",
        "            if metrics[f'val {metric}'] > best_val_metric:\n",
        "                best_val_metric = metrics[f'val {metric}']\n",
        "                corresponding_test_metric = metrics[f'test {metric}']\n",
        "                best_step = step\n",
        "\n",
        "    print(f'Best val {metric}: {best_val_metric:.4f}')\n",
        "    print(f'Corresponding test {metric}: {corresponding_test_metric:.4f}')\n",
        "    print(f'(step {best_step})')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8add8f70",
      "metadata": {
        "id": "8add8f70"
      },
      "source": [
        "This should look quite similar to your standard training loop, but with one notable difference - there are no mini-batches, we are always training on the whole graph. Since the data samples (graph nodes) are not independent, we cannot trivially sample a mini-batch.\n",
        "\n",
        "Now, let's implement a model. Don't forget about skip connections and layer normalization - they can signififcantly boost the performance of a deep learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "d10515d5",
      "metadata": {
        "id": "d10515d5"
      },
      "outputs": [],
      "source": [
        "class FeedForwardModule(nn.Module):\n",
        "    def __init__(self, dim, num_inputs, dropout):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(in_features=num_inputs * dim, out_features=dim)\n",
        "        self.dropout_1 = nn.Dropout(p=dropout)\n",
        "        self.act = nn.GELU()\n",
        "        self.linear_2 = nn.Linear(in_features=dim, out_features=dim)\n",
        "        self.dropout_2 = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear_1(x)\n",
        "        x = self.dropout_1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.dropout_2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResidualModule(nn.Module):\n",
        "    def __init__(self, graph_conv_module, dim, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.normalization = nn.LayerNorm(normalized_shape=dim)\n",
        "        self.graph_conv = graph_conv_module(dim=dim, num_heads=num_heads)\n",
        "        self.feed_forward = FeedForwardModule(dim=dim, num_inputs=2, dropout=dropout)\n",
        "\n",
        "    def forward(self, graph, x):\n",
        "        x_res = self.normalization(x)\n",
        "\n",
        "        x_aggregated = self.graph_conv(graph, x_res)\n",
        "        x_res = torch.cat([x_res, x_aggregated], axis=1)\n",
        "\n",
        "        x_res = self.feed_forward(x_res)\n",
        "\n",
        "        x = x + x_res\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, graph_conv_module, num_layers, input_dim, hidden_dim, output_dim, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.input_linear = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n",
        "        self.input_dropout = nn.Dropout(p=dropout)\n",
        "        self.input_act = nn.GELU()\n",
        "\n",
        "        self.residual_modules = nn.ModuleList(\n",
        "            ResidualModule(graph_conv_module=graph_conv_module, dim=hidden_dim, num_heads=num_heads,\n",
        "                           dropout=dropout)\n",
        "            for _ in range(num_layers)\n",
        "        )\n",
        "\n",
        "        self.output_normalization = nn.LayerNorm(hidden_dim)\n",
        "        self.output_linear = nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
        "\n",
        "    def forward(self, graph, x):\n",
        "        x = self.input_linear(x)\n",
        "        x = self.input_dropout(x)\n",
        "        x = self.input_act(x)\n",
        "\n",
        "        for residual_module in self.residual_modules:\n",
        "            x = residual_module(graph, x)\n",
        "\n",
        "        x = self.output_normalization(x)\n",
        "        logits = self.output_linear(x)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4030a0f4",
      "metadata": {
        "id": "4030a0f4"
      },
      "source": [
        "Now everything is ready - except for the graph convolution module. We will implement several variants of this module, which will constitute the only difference between our GNNs. But first - as a simple baseline - let's implement a graph convolution module that does nothing. It will allow us to see how a graph-agnostic model performs, so we can then compare our GNNs to this baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "2554025c",
      "metadata": {
        "id": "2554025c"
      },
      "outputs": [],
      "source": [
        "class DummyGraphConv(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, graph, x):\n",
        "        return torch.zeros_like(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "bf9f0be4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "1688115c2af94d0ca9e799097a74f51d",
            "b1cfc18dd40b4fcab98cdb056a3ed111",
            "dc0f481108b84edfaea37db34179a06a",
            "ef56aacc8af6438ca6e6606616fc5749",
            "e7eb82a4fa344e93b4cf6b9b98f3fa9b",
            "f901aed04c5d4e8a9580e61d4d612568",
            "9007eeb997ae454cab594b9ff96066bb",
            "696faf9ded4240f89adfe5eb0b6fee2f",
            "5dbba5849bd644bab1a5defe2eade875",
            "0c97fc05d44a456a9b615d1578e339c9",
            "e99087aea97e4d308b1bfd4663d05287"
          ]
        },
        "id": "bf9f0be4",
        "outputId": "036e2400-e835-4c76-a0b9-1fdd2eb9ec93"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1688115c2af94d0ca9e799097a74f51d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best val accuracy: 0.8502\n",
            "Corresponding test accuracy: 0.8386\n",
            "(step 496)\n"
          ]
        }
      ],
      "source": [
        "graph = torch.empty(0)   # We don't care about graph representation for this experiment.\n",
        "\n",
        "run_experiment(graph=graph, features=features, labels=labels,\n",
        "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
        "               graph_conv_module=DummyGraphConv,\n",
        "               device=device, amp=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b11368a",
      "metadata": {
        "id": "1b11368a"
      },
      "source": [
        "Now let's implement some real graph convolutions. Simple graph convolutions can be represented as operations with (sparse) matrices. Thus, they can be implemented in pure PyTorch. We will need the graph adjacency matrix $A$, the graph degree matrix $D$, and the matrix of node representations at layer $l$ $X^l$. Further, let $\\tilde{x_i}^{l}$ be the output of $\\mathrm{Aggregate}$ function at layer $l$ for node $v_i$ and let $\\widetilde{X}^l$ be the matrix of stacked vectors $\\tilde{x_i}^{l}$ for all nodes.\n",
        "\n",
        "For the next couple experiments, assume that the graph argument of the graph convolution forward method is a sparse adjacency matrix of the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "bb74d6e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb74d6e8",
        "outputId": "fde971a8-eddf-434b-b061-c003d02c82b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(indices=tensor([[    0,     0,     0,  ..., 13751, 13751, 13751],\n",
              "                       [  507,  6551,  8210,  ..., 12751, 13019, 13121]]),\n",
              "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
              "       size=(13752, 13752), nnz=491722, layout=torch.sparse_coo)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "graph = torch.sparse_coo_tensor(indices=edges.T, values=torch.ones(len(edges)), size=(len(labels), len(labels)))\n",
        "graph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b90c600c",
      "metadata": {
        "id": "b90c600c"
      },
      "source": [
        "Let's implement a graph convolution that simply takes the mean of neighbors' representations. We can write:\n",
        "\n",
        "$$ \\tilde{x}_i^{l+1} = \\frac{1}{|N(v_i)|} \\sum_{v_j \\in N(v_i)} x_j^l $$\n",
        "\n",
        "This operation can be written in matrix form:\n",
        "\n",
        "$$ \\widetilde{X}^{l+1} = D^{-1} A X^l $$\n",
        "\n",
        "Let's implement it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "789c6382",
      "metadata": {
        "id": "789c6382"
      },
      "outputs": [],
      "source": [
        "class MeanGraphConv(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, graph, x):\n",
        "        # print(x.shape)\n",
        "        diag = torch.diagonal(graph.to_dense())\n",
        "        x_aggregated = diag * graph @ x\n",
        "        # print(x_aggregated.shape)\n",
        "        ######################\n",
        "\n",
        "        return x_aggregated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66d71b93",
      "metadata": {
        "id": "66d71b93"
      },
      "source": [
        "(The computations can be sped up by precomputing $D^{-1} A$, but we won't do it.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "cf23297c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "dfee64e536c142618f47a08087b0f38a",
            "dd165e94387b4f57a097604cd40df3cf",
            "9c733e7ba820440a9720dfe8dd0d7180",
            "2a0c12288cc44ed8b2e1aa04fc76fcdc",
            "e1bd29a966d840d19696df3657f7bbd7",
            "4e5f0398bd2a4919b2aaf7bd90e9a7b1",
            "307bb3d457564f6182299c8506e844fb",
            "8a7110b49db94934ba0a160d2a898c39",
            "c6f770ce58f44be5b9055c8a58834beb",
            "39a8a739b64f4e7cb3f9b46f268d0b6a",
            "6dfa780266bd4017b1813f22c57842c1"
          ]
        },
        "id": "cf23297c",
        "outputId": "6252ad27-5210-46b3-d577-9798a5911599"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfee64e536c142618f47a08087b0f38a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best val accuracy: 0.8485\n",
            "Corresponding test accuracy: 0.8345\n",
            "(step 478)\n"
          ]
        }
      ],
      "source": [
        "run_experiment(graph=graph, features=features, labels=labels,\n",
        "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
        "               graph_conv_module=MeanGraphConv,\n",
        "               device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cefdb56c",
      "metadata": {
        "id": "cefdb56c"
      },
      "source": [
        "As we can see, the accuracy is a lot better than in the previous experiment - our GNN works better than a graph-agnostoc model on this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "698594f2",
      "metadata": {
        "id": "698594f2"
      },
      "source": [
        "Now, let's try another simple GNN variant - this time we will implement a graph convolution proposed in [the GCN paper](https://arxiv.org/abs/1609.02907). The formula is:\n",
        "\n",
        "$$ \\tilde{x}_i^{l+1} = \\sum_{v_j \\in N(v_i)} \\frac{1}{\\sqrt{deg(v_i) deg(v_j)}} x_j^l $$\n",
        "\n",
        "It's very similar to the mean convolution, except we normalize each neighbor's representation not by the degree of the ego node, but by the geometric mean of the degree of the ego node and the neighbor. This operation can be written in matrix form:\n",
        "\n",
        "$$ \\widetilde{X}^{l+1} = D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}} X^l $$\n",
        "\n",
        "Let's implement it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "25de997a",
      "metadata": {
        "id": "25de997a"
      },
      "outputs": [],
      "source": [
        "class GCNGraphConv(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, graph, x):\n",
        "        ### YOUR CODE HERE ###\n",
        "        diag = torch.diagonal(graph.to_dense())\n",
        "        temp = torch.sqrt(diag) * graph.to_dense() * torch.sqrt(diag)\n",
        "        # print(temp.shape)\n",
        "        # print(x.shape)\n",
        "        x_aggregated = temp @ x\n",
        "\n",
        "        ######################\n",
        "\n",
        "        return x_aggregated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01341bf6",
      "metadata": {
        "id": "01341bf6"
      },
      "source": [
        "(The computations can be sped up by precomputing $D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$, but we won't do it.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.diagonal(graph.to_dense())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZrIw2ysrTSI",
        "outputId": "aa14c6da-1564-4e95-83e1-15bb0c5e0d0a"
      },
      "id": "-ZrIw2ysrTSI",
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2YzQFnsnqwhl",
        "outputId": "0d45592f-977a-4d87-996d-6515f914f5a4"
      },
      "id": "2YzQFnsnqwhl",
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "40723c8b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "f77f7d7271ae4b27b517b5b549920a1c",
            "518ac46634ab45028b8a3212d079135e",
            "6022ab8d6f154f07a36dc800ed86a8b1",
            "58d1c533ca114052aaa9f92f6858445d",
            "8deae0930ed043b9b6e1c758119ccdd6",
            "a29fefb149504e319be8e5f229d183e3",
            "622ceaa7faeb4e55bc896085b065ead2",
            "755d11ab28e74fe4a37e64fea01384d0",
            "72cb51c529ee426599c4b57baee3d8d1",
            "26ce4b5040b84532997d4a6509887f09",
            "7c217800ff374ad988924287e6e35f90"
          ]
        },
        "id": "40723c8b",
        "outputId": "409f0ea0-63b2-4140-ec99-f40ea03d342c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f77f7d7271ae4b27b517b5b549920a1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best val accuracy: 0.8386\n",
            "Corresponding test accuracy: 0.8377\n",
            "(step 490)\n"
          ]
        }
      ],
      "source": [
        "run_experiment(graph=graph, features=features, labels=labels,\n",
        "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
        "               graph_conv_module=GCNGraphConv,\n",
        "               device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24658a84",
      "metadata": {
        "id": "24658a84"
      },
      "source": [
        "The results are similar to those in the previous experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d65be773",
      "metadata": {
        "id": "d65be773"
      },
      "source": [
        "Simple graph convolutions can be expressed as matrix operations, and thus, can be implemented in pure PyTorch. However, efficient implementation of more complex graph convolutions requires using specialized libraries. There are two most popular GNN libraries for PyTorch - [PyTorch Geometric (PyG)](https://github.com/pyg-team/pytorch_geometric) and [Deep Graph Library (DGL)](https://www.dgl.ai/). In this seminar, we will be using DGL, because ~it is objectively better~ the instructor likes it more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "863c2414",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "863c2414",
        "outputId": "7a14714a-9f9d-4154-ebf1-f0ed70c0f12a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/cu121/repo.html\n",
            "Collecting dgl\n",
            "  Downloading https://data.dgl.ai/wheels/cu121/dgl-2.1.0%2Bcu121-cp310-cp310-manylinux1_x86_64.whl (467.5 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.3/467.5 MB\u001b[0m \u001b[31m399.8 kB/s\u001b[0m eta \u001b[36m0:06:56\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install dgl -f https://data.dgl.ai/wheels/cu121/repo.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f583f1",
      "metadata": {
        "id": "c9f583f1"
      },
      "outputs": [],
      "source": [
        "import dgl\n",
        "from dgl import ops"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27cd4783",
      "metadata": {
        "id": "27cd4783"
      },
      "source": [
        "There are many features for deep learning on graphs in DGL, but we will only be using two of them - the Graph class, which is obviously used for representing a graph, and the [ops module](https://docs.dgl.ai/api/python/dgl.ops.html), which contains operators for message passing on graphs.\n",
        "\n",
        "First, let's create a graph representation which we will be using in the next few experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b139af27",
      "metadata": {
        "id": "b139af27"
      },
      "outputs": [],
      "source": [
        "graph = dgl.graph((edges[:, 0], edges[:, 1]), num_nodes=len(labels))\n",
        "graph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83d7a06e",
      "metadata": {
        "id": "83d7a06e"
      },
      "source": [
        "Now let's reimplement the mean graph convolution, this time using DGL. For this we will need a certain operation from the ops module - can you guess which one by their names?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50885ea3",
      "metadata": {
        "id": "50885ea3"
      },
      "outputs": [],
      "source": [
        "class DGLMeanGraphConv(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, graph, x):\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "        ######################\n",
        "\n",
        "        return x_aggregated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "484ad64e",
      "metadata": {
        "id": "484ad64e"
      },
      "outputs": [],
      "source": [
        "run_experiment(graph=graph, features=features, labels=labels,\n",
        "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
        "               graph_conv_module=DGLMeanGraphConv,\n",
        "               device=device, amp=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06023062",
      "metadata": {
        "id": "06023062"
      },
      "source": [
        "The results are roughly the same as for the pure PyTorch implementation, but the training is faster (graph message passing operations with DGL a generally faster than PyTorch sparse matrix multiplications, and, further, DGL supports using AMP with most of its operations, while PyTorch does not (yet) allow using AMP with sparse matrix operations)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f578c692",
      "metadata": {
        "id": "f578c692"
      },
      "source": [
        "By simply swapping the ops.copy_u_mean function for the ops.copy_u_max function, we can get another graph convolution that computes the elementwise maximum of neighbors' representations. This one cannot be efficiently implemented in pure PyTorch. Let's see how it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f4af522",
      "metadata": {
        "id": "0f4af522"
      },
      "outputs": [],
      "source": [
        "class DGLMaxGraphConv(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, graph, x):\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "        ######################\n",
        "\n",
        "        return x_aggregated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8135b14",
      "metadata": {
        "id": "f8135b14"
      },
      "outputs": [],
      "source": [
        "run_experiment(graph=graph, features=features, labels=labels,\n",
        "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
        "               graph_conv_module=DGLMaxGraphConv,\n",
        "               device=device)   # This one currently does not work with AMP."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77a66478",
      "metadata": {
        "id": "77a66478"
      },
      "source": [
        "Now, let's reimplement the GCN graph convolution using DGL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5336ec12",
      "metadata": {
        "id": "5336ec12"
      },
      "outputs": [],
      "source": [
        "class DGLGCNGraphConv(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, graph, x):\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "        ######################\n",
        "\n",
        "        return x_aggregated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71d0e78c",
      "metadata": {
        "id": "71d0e78c"
      },
      "source": [
        "(The computations can be sped up by precomputing weights, but we won't do it.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e50b006",
      "metadata": {
        "id": "3e50b006"
      },
      "outputs": [],
      "source": [
        "run_experiment(graph=graph, features=features, labels=labels,\n",
        "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
        "               graph_conv_module=DGLGCNGraphConv,\n",
        "               device=device, amp=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c623c2c",
      "metadata": {
        "id": "7c623c2c"
      },
      "source": [
        "Now let's implement something more complex - the graph convolution proposed in [the GAT paper](https://arxiv.org/abs/1710.10903). This one uses attention (although a very simple version of it). The formulas are:\n",
        "\n",
        "$$ s_{ij} = \\mathrm{LeakyReLU} \\Big( w_1^T x_i^l + w_2^T x_j^l + b \\Big) \\;\\;\\;\\;\\; \\forall (i, j) \\in E_{dir} $$\n",
        "\n",
        "$$ \\Big( p_{ij}: \\; v_j \\in N(v_i) \\Big) = softmax \\Big( s_{ij}: \\; v_j \\in N(v_i) \\Big) \\;\\;\\;\\;\\; \\forall i = 1, ..., n$$\n",
        "\n",
        "This clunky notation means that for each node we take the softmax of attention scores ($s_{ij}$) of its neighbors to get attention probabilities ($p_{ij}$) corresponding to these neighbors. Another way to write it is:\n",
        "\n",
        "$$ p_{ij} = \\frac{ \\exp{(s_{ij})} }{ \\sum_{v_k \\in N(v_i)} \\exp{(s_{ik})} } \\;\\;\\;\\;\\; \\forall (i, j) \\in E_{dir} $$\n",
        "\n",
        "The necessary edge softmax function is available in DGL.\n",
        "\n",
        "$$ \\tilde{x}_i^{l+1} = \\sum_{v_j \\in N(v_i)} p_{ij} x_j^l $$\n",
        "\n",
        "Note that additionally the attention mechanism is multi-headed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36560d4f",
      "metadata": {
        "id": "36560d4f"
      },
      "outputs": [],
      "source": [
        "from dgl.nn.functional import edge_softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd9bf61a",
      "metadata": {
        "id": "fd9bf61a"
      },
      "outputs": [],
      "source": [
        "class DGLGATGraphConv(nn.Module):\n",
        "    def __init__(self, dim, num_heads=4, **kwargs):\n",
        "        super().__init__()\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "        ######################\n",
        "\n",
        "    def forward(self, graph, x):\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "        ######################\n",
        "\n",
        "        return x_aggregated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a402962",
      "metadata": {
        "id": "8a402962"
      },
      "outputs": [],
      "source": [
        "run_experiment(graph=graph, features=features, labels=labels,\n",
        "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
        "               graph_conv_module=DGLGATGraphConv,\n",
        "               device=device, amp=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17344fdb",
      "metadata": {
        "id": "17344fdb"
      },
      "source": [
        "DGL also has an alternative way of creating graph convolutions using dgl.function instead of dgl.ops. It is useful to be familiar with it too, so let's rewrite our mean graph convolution using dgl.function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c6f1a21",
      "metadata": {
        "id": "5c6f1a21"
      },
      "outputs": [],
      "source": [
        "from dgl import function as fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "970e59f2",
      "metadata": {
        "id": "970e59f2"
      },
      "outputs": [],
      "source": [
        "class DGLMeanGraphConvAlt(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, graph, x):\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "        ######################\n",
        "\n",
        "        return x_aggregated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eab41bab",
      "metadata": {
        "id": "eab41bab"
      },
      "outputs": [],
      "source": [
        "run_experiment(graph=graph, features=features, labels=labels,\n",
        "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
        "               graph_conv_module=DGLMeanGraphConvAlt,\n",
        "               device=device, amp=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12199e4d",
      "metadata": {
        "id": "12199e4d"
      },
      "source": [
        "Now, let's see if GNNs can achieve strong performance on a heterophilous graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e036ad",
      "metadata": {
        "id": "e9e036ad"
      },
      "outputs": [],
      "source": [
        "data = datasets.HeterophilousGraphDataset(name='amazon-ratings', root='data/heterophilous-graphs')[0]\n",
        "features = data.x\n",
        "labels = data.y\n",
        "edges = data.edge_index.T\n",
        "\n",
        "# The graph is undirected, but is stored as a directed one (like all graphs in PyTorch Geometric),\n",
        "# so each edge appears twice.\n",
        "print(f'Number of nodes: {len(labels)}')\n",
        "print(f'Number of edges: {len(edges) // 2}')\n",
        "print(f'Average node degree: {len(edges) / len(labels):.2f}')\n",
        "print(f'Number of classes: {len(labels.unique())}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d27682b3",
      "metadata": {
        "id": "d27682b3"
      },
      "outputs": [],
      "source": [
        "# The same split as in the previous seminar, except we now use part of the previous seminar's\n",
        "# train set as a val set.\n",
        "train_idx = torch.where(data.train_mask[:, 0])[0]\n",
        "val_idx = torch.where(data.val_mask[:, 0])[0]\n",
        "test_idx = torch.where(data.test_mask[:, 0])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5122da0a",
      "metadata": {
        "id": "5122da0a"
      },
      "outputs": [],
      "source": [
        "graph = dgl.graph((edges[:, 0], edges[:, 1]), num_nodes=len(labels))\n",
        "graph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cbd83f2",
      "metadata": {
        "id": "1cbd83f2"
      },
      "source": [
        "As always, let's first try a graph-agnostic baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c370cd37",
      "metadata": {
        "id": "c370cd37"
      },
      "outputs": [],
      "source": [
        "run_experiment(graph=graph, features=features, labels=labels,\n",
        "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
        "               graph_conv_module=DummyGraphConv,\n",
        "               device=device, amp=True, num_steps=2500)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a8517ba",
      "metadata": {
        "id": "0a8517ba"
      },
      "source": [
        "Let's see if a GNN with mean graph convolution can achieve significantly better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a45923d5",
      "metadata": {
        "id": "a45923d5"
      },
      "outputs": [],
      "source": [
        "run_experiment(graph=graph, features=features, labels=labels,\n",
        "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
        "               graph_conv_module=DGLMeanGraphConv,\n",
        "               device=device, amp=True, num_steps=2500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3658a18",
      "metadata": {
        "id": "d3658a18"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1688115c2af94d0ca9e799097a74f51d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1cfc18dd40b4fcab98cdb056a3ed111",
              "IPY_MODEL_dc0f481108b84edfaea37db34179a06a",
              "IPY_MODEL_ef56aacc8af6438ca6e6606616fc5749"
            ],
            "layout": "IPY_MODEL_e7eb82a4fa344e93b4cf6b9b98f3fa9b"
          }
        },
        "b1cfc18dd40b4fcab98cdb056a3ed111": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f901aed04c5d4e8a9580e61d4d612568",
            "placeholder": "​",
            "style": "IPY_MODEL_9007eeb997ae454cab594b9ff96066bb",
            "value": "100%"
          }
        },
        "dc0f481108b84edfaea37db34179a06a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_696faf9ded4240f89adfe5eb0b6fee2f",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5dbba5849bd644bab1a5defe2eade875",
            "value": 500
          }
        },
        "ef56aacc8af6438ca6e6606616fc5749": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c97fc05d44a456a9b615d1578e339c9",
            "placeholder": "​",
            "style": "IPY_MODEL_e99087aea97e4d308b1bfd4663d05287",
            "value": " 500/500 [00:07&lt;00:00, 66.21it/s, train accuracy=0.97, val accuracy=0.85, test accuracy=0.84]"
          }
        },
        "e7eb82a4fa344e93b4cf6b9b98f3fa9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f901aed04c5d4e8a9580e61d4d612568": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9007eeb997ae454cab594b9ff96066bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "696faf9ded4240f89adfe5eb0b6fee2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dbba5849bd644bab1a5defe2eade875": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c97fc05d44a456a9b615d1578e339c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e99087aea97e4d308b1bfd4663d05287": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfee64e536c142618f47a08087b0f38a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd165e94387b4f57a097604cd40df3cf",
              "IPY_MODEL_9c733e7ba820440a9720dfe8dd0d7180",
              "IPY_MODEL_2a0c12288cc44ed8b2e1aa04fc76fcdc"
            ],
            "layout": "IPY_MODEL_e1bd29a966d840d19696df3657f7bbd7"
          }
        },
        "dd165e94387b4f57a097604cd40df3cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e5f0398bd2a4919b2aaf7bd90e9a7b1",
            "placeholder": "​",
            "style": "IPY_MODEL_307bb3d457564f6182299c8506e844fb",
            "value": "100%"
          }
        },
        "9c733e7ba820440a9720dfe8dd0d7180": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a7110b49db94934ba0a160d2a898c39",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6f770ce58f44be5b9055c8a58834beb",
            "value": 500
          }
        },
        "2a0c12288cc44ed8b2e1aa04fc76fcdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39a8a739b64f4e7cb3f9b46f268d0b6a",
            "placeholder": "​",
            "style": "IPY_MODEL_6dfa780266bd4017b1813f22c57842c1",
            "value": " 500/500 [00:48&lt;00:00, 10.49it/s, train accuracy=0.97, val accuracy=0.85, test accuracy=0.83]"
          }
        },
        "e1bd29a966d840d19696df3657f7bbd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e5f0398bd2a4919b2aaf7bd90e9a7b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "307bb3d457564f6182299c8506e844fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a7110b49db94934ba0a160d2a898c39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6f770ce58f44be5b9055c8a58834beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39a8a739b64f4e7cb3f9b46f268d0b6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dfa780266bd4017b1813f22c57842c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f77f7d7271ae4b27b517b5b549920a1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_518ac46634ab45028b8a3212d079135e",
              "IPY_MODEL_6022ab8d6f154f07a36dc800ed86a8b1",
              "IPY_MODEL_58d1c533ca114052aaa9f92f6858445d"
            ],
            "layout": "IPY_MODEL_8deae0930ed043b9b6e1c758119ccdd6"
          }
        },
        "518ac46634ab45028b8a3212d079135e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a29fefb149504e319be8e5f229d183e3",
            "placeholder": "​",
            "style": "IPY_MODEL_622ceaa7faeb4e55bc896085b065ead2",
            "value": "100%"
          }
        },
        "6022ab8d6f154f07a36dc800ed86a8b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_755d11ab28e74fe4a37e64fea01384d0",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72cb51c529ee426599c4b57baee3d8d1",
            "value": 500
          }
        },
        "58d1c533ca114052aaa9f92f6858445d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26ce4b5040b84532997d4a6509887f09",
            "placeholder": "​",
            "style": "IPY_MODEL_7c217800ff374ad988924287e6e35f90",
            "value": " 500/500 [01:46&lt;00:00,  4.60it/s, train accuracy=0.97, val accuracy=0.84, test accuracy=0.84]"
          }
        },
        "8deae0930ed043b9b6e1c758119ccdd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a29fefb149504e319be8e5f229d183e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "622ceaa7faeb4e55bc896085b065ead2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "755d11ab28e74fe4a37e64fea01384d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72cb51c529ee426599c4b57baee3d8d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "26ce4b5040b84532997d4a6509887f09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c217800ff374ad988924287e6e35f90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}