{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAahzI9lcLUr"
      },
      "source": [
        "# Practice: question answering with retrieval\n",
        "\n",
        "In this homework you will build a retrieval-based question answering system, one component at a time.\n",
        "\n",
        "_Okay, realistically, there's like, two components, but technically it's one component at a time._\n",
        "\n",
        "\n",
        "![img](https://www.cs.upc.edu/~mlatifi/index_files/qa-logo.jpg)\n",
        "\n",
        "\n",
        "\n",
        "There are two parts to this type of systems: a retriever and a generator.\n",
        "- the retriever subsystem searches for similar texts from a given databse, e.g. wikipedia\n",
        "- the generator uses the texts found by the retriever to generate an answer in natural language\n",
        "\n",
        "_this seminar is based on original notebook by [Oleg Vasilev](https://github.com/Omrigan/)_\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4dtN8dPocLUy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ks8AlEVcLUz"
      },
      "source": [
        "### Part 0: the dataset\n",
        "\n",
        "Before we train anything, let's take a look at the question answering data that we can use. There are several popilar datasets, e.g. TriviaQA for trivia questions or GSM8K for math. Today's data is Stanford Question Answering Dataset (SQuAD). Given a paragraph of text and a question, our model's task is to select a snippet that answers the question.\n",
        "\n",
        "We are not going to solve the full task today. Instead, we'll train a model to __select the sentence containing answer__ among several options.\n",
        "\n",
        "As usual, you are given an utility module with data reader and some helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iWRei-HxcLU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "161f780a-d982-48f6-9f83-49f50a4570ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-27 03:33:45--  https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week_extra/retrieval/data.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3553 (3.5K) [text/plain]\n",
            "Saving to: ‘data.py’\n",
            "\n",
            "\rdata.py               0%[                    ]       0  --.-KB/s               \rdata.py             100%[===================>]   3.47K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-12-27 03:33:45 (42.4 MB/s) - ‘data.py’ saved [3553/3553]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "%pip install --quiet transformers==4.36.2 accelerate==0.24.0 sentencepiece==0.1.99 optimum==1.13.2 auto-gptq==0.4.2\n",
        "!wget https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week_extra/retrieval/data.py -O data.py\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O squad-v2.0.json 2> log\n",
        "\n",
        "import data\n",
        "# backup download link: https://www.dropbox.com/s/q4fuihaerqr0itj/squad.tar.gz?dl=1\n",
        "train, test = data.build_dataset('./squad-v2.0.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KvzzSgVicLU0",
        "outputId": "1e8245b6-c855-4ee1-fa25-7e77f543d11e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QUESTION Where did Beyonce get her name from? \n",
            "\n",
            "TEXT SENTENCES\n",
            "[ ] Beyoncé Giselle Knowles was born in Houston, Texas, to Celestine Ann \"Tina\" Knowles (née Beyincé), a hairdresser and salon owner, and Mathew Knowles, a Xerox sales manager.\n",
            "[v] Beyoncé's name is a tribute to her mother's maiden name.\n",
            "[ ] Beyoncé's younger sister Solange is also a singer and a former member of Destiny's Child.\n",
            "[ ] Mathew is African-American, while Tina is of Louisiana Creole descent (with African, Native American, French, Cajun, and distant Irish and Spanish ancestry).\n",
            "[ ] Through her mother, Beyoncé is a descendant of Acadian leader Joseph Broussard.\n",
            "[ ] She was raised in a Methodist household.\n"
          ]
        }
      ],
      "source": [
        "pid, question, options, correct_indices, wrong_indices = train.iloc[40]\n",
        "print('QUESTION', question, '\\n')\n",
        "print('TEXT SENTENCES')\n",
        "for i, cand in enumerate(options):\n",
        "    print(['[ ]', '[v]'][i in correct_indices], cand)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGxVKpjAcLU1"
      },
      "source": [
        "### Pre-trained BERT\n",
        "_(but you guessed it)_\n",
        "\n",
        "We've already solved quite a few tasks from scratch, training our own embeddings and convolutional/recurrent layers. However, one can often achieve higher quality by using pre-trained models. We will default to the good ol' [BERT](https://arxiv.org/abs/1810.04805), though, you are free to use any [other model](https://huggingface.co/models) as you see fit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iRAdNj2IcLU1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "\n",
        "model_name = 'sentence-transformers/bert-base-nli-mean-tokens'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "bert = AutoModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kAQHqgW_cLU1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c328788-f9eb-4329-8e25-db0765905011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ],
      "source": [
        "# just a reminder: here's how it works\n",
        "dummy_lines = [\n",
        "    \"How old are you?\",                                                 # 0\n",
        "    \"In what mythology do two canines watch over the Chinvat Bridge?\",  # 1\n",
        "    \"I'm sorry, okay, I'm not perfect, but I'm trying.\",                # 2\n",
        "    \"What is your age?\",                                                # 3\n",
        "    \"Beware, for I am fearless, and therefore powerful.\",               # 4\n",
        "]\n",
        "\n",
        "with torch.no_grad():\n",
        "    batch_tensors = tokenizer(dummy_lines, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    out = bert(**batch_tensors)\n",
        "    token_embs = out.last_hidden_state\n",
        "    cls_embs = out.pooler_output\n",
        "    del out\n",
        "\n",
        "\n",
        "mask = batch_tensors['attention_mask'][..., None].to(torch.float32)\n",
        "naive_phrase_embs = (token_embs * mask).sum(1) / mask.sum(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eYLyOgvzcLU3",
        "outputId": "5ebeeb32-15b4-424a-dd00-0b1beb41299e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7b3c15ba1a80>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.title('phrase similarity')\n",
        "plt.imshow((naive_phrase_embs @ naive_phrase_embs.t()).cpu().data.numpy(), interpolation='none', cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apBCmWWgcLU4"
      },
      "source": [
        "As you can see, __the strongest similarity is between lines 0 and 3__. Indeed they correspond to \"How old are you?\" and \"What is your age?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3qa0HGrcLU4"
      },
      "source": [
        "### Retriever Model (2 points)\n",
        "\n",
        "Our goal for today is to build a model that measures similarity between question and answer. In particular, it maps both question and answer into fixed-size vectors such that:\n",
        "\n",
        "Our model is a pair of $V_q(q)$ and $V_a(a)$ - networks that turn phrases into vectors.\n",
        "\n",
        "__Objective:__ Question vector $V_q(q)$ should be __closer__ to correct answer vectors $V_a(a^+)$ than to incorrect ones $V_a(a^-)$ .\n",
        "\n",
        "Both vectorizers can be anything you wish. For starters, let's use a couple of dense layers on top of the pre-trained encoder.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "azvTj2dlCNRF"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2LnBd1IdcLU4"
      },
      "outputs": [],
      "source": [
        "class Vectorizer(nn.Module):\n",
        "    def __init__(self, hid_size=256, bert=bert):\n",
        "        \"\"\" A small feedforward network on top of pre-trained encoder. 2-3 layers should be enough \"\"\"\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.hid_size = hid_size\n",
        "\n",
        "        # define a few layers to be applied on top of pre-trained BERT\n",
        "        # note: please make sure your final layer comes with _linear_ activation\n",
        "        interm_size = bert.pooler.dense.out_features\n",
        "        self.fc_1 = nn.Linear(interm_size, hid_size*2)\n",
        "        self.fc_2 = nn.Linear(hid_size*2, hid_size*2)\n",
        "        self.fc_3 = nn.Linear(hid_size*2, hid_size)\n",
        "        self.dropout_1 = nn.Dropout(p=0.35)\n",
        "        self.dropout_2 = nn.Dropout(p=0.35)\n",
        "        self.activation_1 = nn.ReLU()\n",
        "        self.activation_2 = nn.ReLU()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input_phrases):\n",
        "        \"\"\"\n",
        "        Apply vectorizer. Use dropout and any other hacks at will.\n",
        "        :param input_phrases: a list of strings, [batch_size]\n",
        "        :returns: predicted phrase vectors, [batch_size, output_size]\n",
        "\n",
        "        Note: you may want to use dropouts.\n",
        "        if self.training:\n",
        "          <something>\n",
        "\n",
        "        Note 2: you may also want to use with torch.no_grad to avoid training BERT for your first attempts\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # In fact, please DO use at least 10% dropout!\n",
        "        # <YOUR CODE>\n",
        "        # return <...>\n",
        "        with torch.no_grad():\n",
        "            batch_tensors = tokenizer(input_phrases, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "            batch_tensors = {k: v.to(device) for k, v in batch_tensors.items()}\n",
        "            out = self.bert(**batch_tensors)\n",
        "            token_embs = out.last_hidden_state\n",
        "            cls_embs = out.pooler_output\n",
        "            # print(token_embs.shape, cls_embs.shape)\n",
        "            del out\n",
        "            # mask = batch_tensors['attention_mask'][..., None].to(torch.float32)\n",
        "            # naive_phrase_embs = (token_embs * mask).sum(1) / mask.sum(1)\n",
        "            # full_embs = torch.concat([naive_phrase_embs, cls_embs], axis=1)\n",
        "        # print(full_embs.shape)\n",
        "        full_embs = cls_embs\n",
        "        x = self.fc_1(full_embs)\n",
        "        x = self.activation_1(x)\n",
        "        x = self.dropout_1(x)\n",
        "        x = self.fc_2(x)\n",
        "        x = self.activation_2(x)\n",
        "        x = self.dropout_2(x)\n",
        "        x = self.fc_3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bmlIxPRQcLU5"
      },
      "outputs": [],
      "source": [
        "question_vectorizer = Vectorizer().to(device)\n",
        "answer_vectorizer = Vectorizer().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "krX1gnD6cLU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a465ec26-f175-4052-c3f2-cb6f4c328ed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Well done!\n"
          ]
        }
      ],
      "source": [
        "question_vectorizer.train(False)\n",
        "out1 = question_vectorizer(dummy_lines)\n",
        "out2 = question_vectorizer(dummy_lines)\n",
        "assert tuple(out1.shape) == (5, question_vectorizer.hid_size)\n",
        "assert torch.allclose(out1, out2, atol=1e-5, rtol=0), \"Make sure your model disables dropout if training == False\"\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lUHY7LdcLU5"
      },
      "source": [
        "### Retriever training: minibatches\n",
        "\n",
        "Our model learns on triples $(q, a^+, a^-)$:\n",
        "* q - <b>q</b>uestion\n",
        "* (a+) - correct <b>a</b>nswer\n",
        "* (a-) - wrong <b>a</b>nswer\n",
        "\n",
        "Below you will find a generator that samples such triples from data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GO64sZ3AcLU6"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def iterate_minibatches(data, batch_size, shuffle=True, cycle=False):\n",
        "    \"\"\"\n",
        "    Generates minibatches of triples: {questions, correct answers, wrong answers}\n",
        "    If there are several wrong (or correct) answers, picks one at random.\n",
        "    \"\"\"\n",
        "    indices = np.arange(len(data))\n",
        "    while True:\n",
        "        if shuffle:\n",
        "            indices = np.random.permutation(indices)\n",
        "        for batch_start in range(0, len(indices), batch_size):\n",
        "            batch_indices = indices[batch_start: batch_start + batch_size]\n",
        "            batch = data.iloc[batch_indices]\n",
        "            questions = batch['question'].values\n",
        "            correct_answers = np.array([\n",
        "                row['options'][random.choice(row['correct_indices'])]\n",
        "                for i, row in batch.iterrows()\n",
        "            ])\n",
        "            wrong_answers = np.array([\n",
        "                row['options'][random.choice(row['wrong_indices'])]\n",
        "                for i, row in batch.iterrows()\n",
        "            ])\n",
        "\n",
        "            yield {\n",
        "                'questions' : questions,\n",
        "                'correct_answers': correct_answers,\n",
        "                'wrong_answers': wrong_answers,\n",
        "            }\n",
        "        if not cycle:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wcnGu02BcLU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f62b5218-1f72-4e2f-e8fb-91fddd836dba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'questions': array(['how does a user store electronic mail?',\n",
            "       'How many new data centers did IBM commit to building?',\n",
            "       'What age are the cadets in the Canadian Cadet Movement?'],\n",
            "      dtype=object), 'correct_answers': array(['A mailbox provider is an organization that provides services for hosting electronic mail domains with access to storage for mail boxes.',\n",
            "       'It also committed to a $1.2bn plus expansion of its data center and cloud-storage business, including the development of 15 new data centers.',\n",
            "       'The Cadet Organizations Administration and Training Service (COATS) consists of officers and non-commissioned members who conduct training, safety, supervision and administration of nearly 60,000 cadets aged 12 to 18 years in the Canadian Cadet Movement.'],\n",
            "      dtype='<U254'), 'wrong_answers': array(['It provides email servers to send, receive, accept, and store email for end users or other organizations.',\n",
            "       'After ten successive quarters of flat or sliding sales under Chief Executive Virginia Rometty IBM is being forced to look at new approaches.',\n",
            "       'Members of the Reserve Force Sub-Component COATS who are not employed part-time (Class A) or full-time (Class B) may be held on the \"Cadet Instructor Supplementary Staff List\" (CISS List) in anticipation of employment in the same manner as other reservists are held as members of the Supplementary Reserve.'],\n",
            "      dtype='<U306')}\n"
          ]
        }
      ],
      "source": [
        "dummy_batch = next(iterate_minibatches(train.sample(3), 3))\n",
        "print(dummy_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F09wfUH9cLU6"
      },
      "source": [
        "### Retriever training: loss function (2 points)\n",
        "We want our vectorizers to put correct answers closer to question vectors and incorrect answers farther away from them. One way to express this is to use is Pairwise Hinge Loss _(aka Triplet Loss)_.\n",
        "\n",
        "$$ L = \\frac 1N \\underset {q, a^+, a^-} \\sum max(0, \\space \\delta - sim[V_q(q), V_a(a^+)] + sim[V_q(q), V_a(a^-)] )$$\n",
        "\n",
        ", where\n",
        "* sim[a, b] is some similarity function: dot product, cosine or negative distance\n",
        "* δ - loss hyperparameter, e.g. δ=1.0. If sim[a, b] is linear in b, all δ > 0 are equivalent.\n",
        "\n",
        "\n",
        "This reads as __Correct answers must be closer than the wrong ones by at least δ.__\n",
        "\n",
        "![img](https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/margin.png)\n",
        "<center>_image: question vector is green, correct answers are blue, incorrect answers are red_</center>\n",
        "\n",
        "\n",
        "Note: in effect, we train a Deep Semantic Similarity Model [DSSM](https://www.microsoft.com/en-us/research/project/dssm/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nlV8YSewcLU7"
      },
      "outputs": [],
      "source": [
        "def similarity(a, b):\n",
        "    \"\"\" Dot product as a similarity function \"\"\"\n",
        "    dot = a @ b.T\n",
        "    if a.shape[0]==1:\n",
        "        return dot\n",
        "\n",
        "    return torch.diag(dot)\n",
        "\n",
        "def compute_loss(question_vectors, correct_answer_vectors, wrong_answer_vectors, delta=1.0):\n",
        "    \"\"\"\n",
        "    Compute the triplet loss as per formula above.\n",
        "    Use similarity function above for  sim[a, b]\n",
        "    :param question_vectors: float32[batch_size, vector_size]\n",
        "    :param correct_answer_vectors: float32[batch_size, vector_size]\n",
        "    :param wrong_answer_vectors: float32[batch_size, vector_size]\n",
        "    :returns: loss for every row in batch, float32[batch_size]\n",
        "    Hint: you can compute max(0, *) using torch.relu :)\n",
        "    \"\"\"\n",
        "    m = question_vectors.shape[0]\n",
        "    pos_sim = similarity(question_vectors, correct_answer_vectors)\n",
        "    neg_sim = similarity(question_vectors, wrong_answer_vectors)\n",
        "    # print(pos_sim, neg_sim)\n",
        "    interm = delta - pos_sim + neg_sim\n",
        "    # print(interm)\n",
        "    ans = torch.relu(interm)\n",
        "    # print(ans)\n",
        "    return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj3XOWpTCNRK",
        "outputId": "1389ce0c-17b7-4efa-d4d6-1e26c03a9b1b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "torch.relu(torch.tensor([-0.9, 1.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pzivm0jqcLU7"
      },
      "outputs": [],
      "source": [
        "dummy_v1 = torch.tensor([[0.1, 0.2, -1], [-1.2, 0.6, 1.0]], dtype=torch.float32)\n",
        "dummy_v2 = torch.tensor([[0.9, 2.1, -6.6], [0.1, 0.8, -2.2]], dtype=torch.float32)\n",
        "dummy_v3 = torch.tensor([[-4.1, 0.1, 1.2], [0.3, -1, -2]], dtype=torch.float32)\n",
        "# print(similarity(dummy_v1, dummy_v2).data.numpy())\n",
        "# print(compute_loss(dummy_v1, dummy_v2, dummy_v3, delta=5.0).data.numpy())\n",
        "assert np.allclose(similarity(dummy_v1, dummy_v2).data.numpy(), [7.11, -1.84])\n",
        "assert np.allclose(compute_loss(dummy_v1, dummy_v2, dummy_v3, delta=5.0).data.numpy(), [0.0, 3.88])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc30eCwKcLU7"
      },
      "source": [
        "Once loss is working, let's train our model by our usual means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7n0_WO7CNRL",
        "outputId": "64f786df-ee5a-4c4a-f967-60781e070c88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'questions': array(['how does a user store electronic mail?',\n",
              "        'How many new data centers did IBM commit to building?',\n",
              "        'What age are the cadets in the Canadian Cadet Movement?'],\n",
              "       dtype=object),\n",
              " 'correct_answers': array(['A mailbox provider is an organization that provides services for hosting electronic mail domains with access to storage for mail boxes.',\n",
              "        'It also committed to a $1.2bn plus expansion of its data center and cloud-storage business, including the development of 15 new data centers.',\n",
              "        'The Cadet Organizations Administration and Training Service (COATS) consists of officers and non-commissioned members who conduct training, safety, supervision and administration of nearly 60,000 cadets aged 12 to 18 years in the Canadian Cadet Movement.'],\n",
              "       dtype='<U254'),\n",
              " 'wrong_answers': array(['It provides email servers to send, receive, accept, and store email for end users or other organizations.',\n",
              "        'After ten successive quarters of flat or sliding sales under Chief Executive Virginia Rometty IBM is being forced to look at new approaches.',\n",
              "        'Members of the Reserve Force Sub-Component COATS who are not employed part-time (Class A) or full-time (Class B) may be held on the \"Cadet Instructor Supplementary Staff List\" (CISS List) in anticipation of employment in the same manner as other reservists are held as members of the Supplementary Reserve.'],\n",
              "       dtype='<U306')}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "dummy_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rXhjfHCLcLU8"
      },
      "outputs": [],
      "source": [
        "# we also compute recall: the rate at which a^+ is closer to q than a^-\n",
        "def get_recall(questions, correct_answers, wrong_answers):\n",
        "\n",
        "    v_questions = question_vectorizer(questions.tolist())\n",
        "    v_correct = answer_vectorizer(correct_answers.tolist())\n",
        "    v_incorrect = answer_vectorizer(wrong_answers.tolist())\n",
        "\n",
        "    correct_is_closer = similarity(v_questions, v_correct) > similarity(v_questions, v_incorrect)\n",
        "\n",
        "    recall = torch.mean(correct_is_closer.to(torch.float32)).item()\n",
        "    return recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MOdhf_z4cLU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c28b26f-25d9-41fd-dd3e-5dc051df3895"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6666666865348816"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# make sure it works\n",
        "get_recall(**dummy_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KArsnvUpcLU9"
      },
      "source": [
        "### Training loop (1 point)\n",
        "\n",
        "Just as we always do, we can now train DSSM on minibatches and periodically measure recall on validation data.\n",
        "\n",
        "\n",
        "__Note 1:__ triplet loss training may be very sensitive to the choice of batch size. Small batch size may decrease model quality because there are less negative to consider.\n",
        "\n",
        "__Note 2:__ here we use the same dataset as __\"test set\"__ and __\"validation (dev) set\"__.\n",
        "\n",
        "In any serious scientific experiment, those must be two separate sets. Validation is for hyperparameter tuning and test is for final eval only.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NTKNwrXuCNRM"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "BATCH_SIZE = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ol_qCuFYCNRM"
      },
      "outputs": [],
      "source": [
        "bert = bert.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gDaKqIdqcLU9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Initialize model, optimizers, anything else you want\n",
        "question_vectorizer = Vectorizer().to(device)\n",
        "answer_vectorizer = Vectorizer().to(device)\n",
        "\n",
        "question_opt = torch.optim.AdamW(question_vectorizer.parameters(), lr=3e-02)\n",
        "answer_opt = torch.optim.AdamW(answer_vectorizer.parameters(), lr=3e-02)\n",
        "\n",
        "ewma = lambda x, span: pd.DataFrame({'x': x})['x'].ewm(span=span).mean().values\n",
        "dev_batches = iterate_minibatches(test, batch_size=BATCH_SIZE, cycle=True)\n",
        "loss_history = []\n",
        "dev_recall_history = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bert.pooler.dense.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOS8LE546aBv",
        "outputId": "5fa6b849-9941-4c72-a85e-d5eb85ac7712"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0013, -0.0381, -0.0158,  ...,  0.0244, -0.0008,  0.0240],\n",
            "        [ 0.0020,  0.0151,  0.0033,  ...,  0.0180, -0.0023,  0.0231],\n",
            "        [-0.0386,  0.0145,  0.0621,  ...,  0.0374, -0.0105, -0.0395],\n",
            "        ...,\n",
            "        [-0.0111,  0.0136,  0.0541,  ...,  0.0666,  0.0017, -0.0090],\n",
            "        [ 0.0001,  0.0024, -0.0125,  ...,  0.0046, -0.0014, -0.0079],\n",
            "        [ 0.0415,  0.0751,  0.0305,  ...,  0.0317,  0.0479,  0.0080]],\n",
            "       device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bert.pooler.dense.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWPkrhqr6q4G",
        "outputId": "474cfffe-dbf2-49cf-d822-5433ffbb9cca"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0013, -0.0381, -0.0158,  ...,  0.0244, -0.0008,  0.0240],\n",
            "        [ 0.0020,  0.0151,  0.0033,  ...,  0.0180, -0.0023,  0.0231],\n",
            "        [-0.0386,  0.0145,  0.0621,  ...,  0.0374, -0.0105, -0.0395],\n",
            "        ...,\n",
            "        [-0.0111,  0.0136,  0.0541,  ...,  0.0666,  0.0017, -0.0090],\n",
            "        [ 0.0001,  0.0024, -0.0125,  ...,  0.0046, -0.0014, -0.0079],\n",
            "        [ 0.0415,  0.0751,  0.0305,  ...,  0.0317,  0.0479,  0.0080]],\n",
            "       device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuDUsgjrcLU9"
      },
      "outputs": [],
      "source": [
        "# infinite training loop. Stop it manually or implement early stopping\n",
        "\n",
        "for batch in iterate_minibatches(train, batch_size=BATCH_SIZE, cycle=True):\n",
        "    # print(batch.keys())\n",
        "    # break\n",
        "    # Perform one training step\n",
        "    # <YOUR CODE>\n",
        "    answer_opt.zero_grad()\n",
        "    question_opt.zero_grad()\n",
        "\n",
        "    question_vectorizer.train(True)\n",
        "    answer_vectorizer.train(True)\n",
        "\n",
        "    questions = batch['questions'].tolist()\n",
        "    correct_answers = batch['correct_answers'].tolist()\n",
        "    wrong_answers = batch['wrong_answers'].tolist()\n",
        "    q_embs = question_vectorizer(questions).to(device)\n",
        "    c_embs = answer_vectorizer(correct_answers).to(device)\n",
        "    w_embs = answer_vectorizer(wrong_answers).to(device)\n",
        "\n",
        "    # v_questions = question_vectorizer(questions.tolist())\n",
        "    # v_correct = answer_vectorizer(correct_answers.tolist())\n",
        "    # v_incorrect = answer_vectorizer(wrong_answers.tolist())\n",
        "    loss_t = compute_loss(q_embs, c_embs, w_embs, delta=5.0)\n",
        "    loss = loss_t.mean()\n",
        "    loss.backward()\n",
        "\n",
        "    answer_opt.step()\n",
        "    question_opt.step()\n",
        "\n",
        "    loss_history.append(float(loss))\n",
        "\n",
        "    if len(loss_history) % 50 == 0:\n",
        "        # measure dev recall = P(correct_is_closer_than_wrong | q, a+, a-)\n",
        "        question_vectorizer.train(False)\n",
        "        answer_vectorizer.train(False)\n",
        "        dev_batch = next(dev_batches)\n",
        "\n",
        "        recall_t = get_recall(**dev_batch)\n",
        "\n",
        "        dev_recall_history.append(recall_t)\n",
        "\n",
        "    if len(loss_history) % 50 == 0:\n",
        "        question_vectorizer.train(False)\n",
        "        answer_vectorizer.train(False)\n",
        "\n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=[12, 6])\n",
        "        plt.subplot(1, 2, 1), plt.title('train loss (hinge)'), plt.grid()\n",
        "        plt.scatter(np.arange(len(loss_history)), loss_history, alpha=0.1)\n",
        "        plt.plot(ewma(loss_history, span=100))\n",
        "        plt.subplot(1, 2, 2), plt.title('dev recall (1 correct vs 1 wrong)'), plt.grid()\n",
        "        dev_time = np.arange(1, len(dev_recall_history) + 1) * 100\n",
        "        plt.scatter(dev_time, dev_recall_history, alpha=0.1)\n",
        "        plt.plot(dev_time, ewma(dev_recall_history, span=10))\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "kYLKsDNKcLU9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "3f3f3dce-9c7c-407e-d704-7c0503988fe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean recall: 0.748046875\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-70659c809967>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean recall:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_recall_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_recall_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.85\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Please train for at least 85% recall on test set. \"\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                   \u001b[0;34m\"You may need to change vectorizer model for that.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Well done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Please train for at least 85% recall on test set. You may need to change vectorizer model for that."
          ]
        }
      ],
      "source": [
        "print(\"Mean recall:\", np.mean(dev_recall_history[-10:]))\n",
        "assert np.mean(dev_recall_history[-10:]) > 0.85, \"Please train for at least 85% recall on test set. \"\\\n",
        "                                                  \"You may need to change vectorizer model for that.\"\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "YMsVSsaScLU-"
      },
      "source": [
        "# Retriever evaluation (2 point)\n",
        "\n",
        "Let's see how well does our model perform on actual question answering.\n",
        "\n",
        "Given a question and a set of possible answers, pick answer with highest similarity to estimate accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "drbFaxidcLU-"
      },
      "outputs": [],
      "source": [
        "# optional: any additional preparations, e.g. build index\n",
        "# <...>\n",
        "\n",
        "def select_best_answer(question, possible_answers):\n",
        "    \"\"\"\n",
        "    Predicts which answer best fits the question\n",
        "    :param question: a single string containing a question\n",
        "    :param possible_answers: a list of strings containing possible answers\n",
        "    :returns: integer - the index of best answer in possible_answer\n",
        "    \"\"\"\n",
        "    # print(question)\n",
        "    # print(len(possible_answers))\n",
        "    q_embs = question_vectorizer([question])\n",
        "    a_embs = answer_vectorizer(possible_answers)\n",
        "    # print(q_embs.shape, a_embs.shape)\n",
        "    similarities = similarity(q_embs, a_embs)\n",
        "    # print(similarities.shape)\n",
        "    return torch.argmax(similarities).item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "9aLwahCxcLU-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "6c450055-0fce-4059-cb2d-0f7460a83e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:03<00:00, 31.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.54000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-2f253379e99b>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m ])\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %0.5f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.65\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"we need more accuracy!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Great job!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: we need more accuracy!"
          ]
        }
      ],
      "source": [
        "predicted_answers = [\n",
        "    select_best_answer(question, possible_answers)\n",
        "    for i, (question, possible_answers) in tqdm(list(test[['question', 'options']].iterrows())[0:100], total=100)\n",
        "]\n",
        "\n",
        "accuracy = np.mean([\n",
        "    answer in correct_ix\n",
        "    for answer, correct_ix in zip(predicted_answers, test['correct_indices'].values)\n",
        "])\n",
        "print(\"Accuracy: %0.5f\" % accuracy)\n",
        "assert accuracy > 0.65, \"we need more accuracy!\"\n",
        "print(\"Great job!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_answers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcEWlUw5Ocp9",
        "outputId": "975aa7ea-5de1-4668-c6a6-49034cad3a51"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 3,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 6,\n",
              " 1,\n",
              " 6,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 7,\n",
              " 7,\n",
              " 2,\n",
              " 0,\n",
              " 7,\n",
              " 6,\n",
              " 0,\n",
              " 3,\n",
              " 3,\n",
              " 7,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 3,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 5,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 1,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 4,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3ODWG4kcLU_"
      },
      "outputs": [],
      "source": [
        "def draw_results(question, possible_answers, predicted_index, correct_indices):\n",
        "    print(\"Q:\", question, end='\\n\\n')\n",
        "    for i, answer in enumerate(possible_answers):\n",
        "        print(\"#%i: %s %s\" % (i, '[*]' if i == predicted_index else '[ ]', answer))\n",
        "\n",
        "    print(\"\\nVerdict:\", \"CORRECT\" if predicted_index in correct_indices else \"INCORRECT\",\n",
        "          \"(ref: %s)\" % correct_indices, end='\\n' * 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLbpRSSmcLU_",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "for i in [1, 100, 1000, 2000, 3000, 4000, 5000]:\n",
        "    draw_results(test.iloc[i].question, test.iloc[i].options,\n",
        "                 predicted_answers[i], test.iloc[i].correct_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki4qVJo_cLU_"
      },
      "outputs": [],
      "source": [
        "question = \"What is my name?\" # your question here!\n",
        "possible_answers = [\n",
        "    <...>\n",
        "    # ^- your options.\n",
        "]\n",
        "predicted answer = select_best_answer(question, possible_answers)\n",
        "\n",
        "draw_results(question, possible_answers,\n",
        "             predicted_answer, [0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw0l5dyAcLU_"
      },
      "source": [
        "### Part 2: to prompt a generator (3 points)\n",
        "\n",
        "You have built a model that can select the most relevant sentence from a text document. However, this is still not the same as question answering - at least not how humans understand it. The full question answering system shoud answer your question in a dialogue - and perhap even let you ask follow-up questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFCrKiUgcLVA"
      },
      "outputs": [],
      "source": [
        "# note: you may want to save pre-trained retriever and restart to free memory\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "model_name = 'TheBloke/Mistral-7B-Instruct-v0.2-GPTQ'  # feel free to choose any other model\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map='auto',\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b_lFbiPcLVA",
        "outputId": "13e16785-ac43-4fd5-a73a-1c4714c9b513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QUESTION What was the first album Beyoncé released as a solo artist? \n",
            "\n",
            "TEXT SENTENCES\n",
            "[ ] Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress.\n",
            "[ ] Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child.\n",
            "[ ] Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time.\n",
            "[v] Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n"
          ]
        }
      ],
      "source": [
        "pid, question, options, correct_indices, wrong_indices = train.iloc[10]\n",
        "print('QUESTION', question, '\\n')\n",
        "print('TEXT SENTENCES')\n",
        "for i, cand in enumerate(options):\n",
        "    print(['[ ]', '[v]'][i in correct_indices], cand)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhQ9r0RycLVA",
        "outputId": "06b3c16d-5872-4b6b-df36-50893fa090d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The first album Beyoncé released as a solo artist was \"Dangerously in Love\" in 2003.</s>\n"
          ]
        }
      ],
      "source": [
        "question_and_info = \"\"\"\n",
        "Question: What was the first album Beyoncé released as a solo artist?\n",
        "\n",
        "Information: Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
        "\"\"\"  # <-- you should un-hardcode this\n",
        "\n",
        "input_ids = tokenizer.apply_chat_template([\n",
        "    dict(role='user', content=\"\"\"You will be given a question and a piece of information that contains the answer. Please reply with a short informal answer to a question based on the information given.\"\"\".strip()),\n",
        "    dict(role='assistant', content=\"\"\"Okay, what question would you like me to answer?\"\"\"),\n",
        "    dict(role='user', content=question_and_info.strip())], return_tensors='pt').to(device)\n",
        "# details: https://huggingface.co/docs/transformers/main/en/chat_templating ; only used for chat/instruct models\n",
        "\n",
        "output_tokens = model.generate(input_ids, attention_mask=torch.ones_like(input_ids),\n",
        "                               max_new_tokens=64, do_sample=True, temperature=0.8, top_p=0.9)\n",
        "print(tokenizer.decode(output_tokens[0, input_ids.shape[1]:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35yibHtVcLVB"
      },
      "source": [
        "### Optional tasks for bonus points\n",
        "\n",
        "There are many ways to improve our question answering model. Here's a bunch of things you can do to increase your understanding and get bonus points.\n",
        "\n",
        "### 1.  Hard Negatives (2+ pts)\n",
        "\n",
        "Not all wrong answers are equally wrong. As the training progresses, _most negative examples $a^-$ will be to easy._ So easy in fact, that loss function and gradients on such negatives is exactly __0.0__. To improve training efficiency, one can __mine hard negative samples__.\n",
        "\n",
        "Given a list of answers,\n",
        "* __Hard negative__ is the wrong answer with highest similarity with question,\n",
        "\n",
        "$$a^-_{hard} = \\underset {a^-} {argmax} \\space sim[V_q(q), V_a(a^-)]$$\n",
        "\n",
        "* __Semi-hard negative__ is the one with highest similarity _among wrong answers that are farther than positive one. This option is more useful if some wrong answers may actually be mislabelled correct answers.\n",
        "\n",
        "* One can also __sample__ negatives proportionally to $$P(a^-_i) \\sim e ^ {sim[V_q(q), V_a(a^-_i)]}$$\n",
        "\n",
        "\n",
        "The task is to implement at least __hard negative__ sampling and apply it for model training.\n",
        "\n",
        "\n",
        "### 2. Better prompting (2+ pts)\n",
        "\n",
        "In the previous example, we manually engineer a prompt for an LLM to solve produce an answer. However, by this point you know multiple ways to make LLM do your bidding. In this assignment, you should try at least some of them:\n",
        "- try few-shot learning with several handcrafted examples (or hand-picked model inputs)\n",
        "- compare several instruct and/or non-instruct models; for non-instruct models\n",
        "  - please not that you should not use apply_chat_template for non-instruct models\n",
        "- provide some means of quality evaluation to compare your approach against the default one\n",
        "\n",
        "At the minimum, several (10-20) side-by-side examples would do the trick. However, we'd appreciate creative means of evaluation here (crowdsourcing, asking another LM, anything exotic as long as you can explain it).\n",
        "\n",
        "### 3. Search engine (3+ pts)\n",
        "\n",
        "Our basic model only selects answers from 2-5 available sentences in paragraph. You can extend it to search over __the whole dataset__. All sentences in all other paragraphs are viable answers.\n",
        "\n",
        "The goal is to train such a model and use it to __quickly find top-10 answers from the whole set__.\n",
        "\n",
        "* You can ask such model a question of your own making - to see which answers it can find among the entire training dataset or even the entire wikipedia.\n",
        "* Searching for top-K neighbors is easier if you use specialized methods: [KD-Tree](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html) or [HNSW](https://github.com/nmslib/hnswlib).\n",
        "* This task is much easier to train if you use hard or semi-hard negatives. You can even find hard negatives for one question from correct answers to other questions in batch - do so in-graph for maximum efficiency. See [1.] for more details.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}